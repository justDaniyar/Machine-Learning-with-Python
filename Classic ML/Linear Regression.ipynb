{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472c4f22-91d5-4da5-863c-109a70fb0854",
   "metadata": {},
   "source": [
    "## 1. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead492e-8e91-46f5-a1b1-6f593f5f5b5f",
   "metadata": {},
   "source": [
    "**Linear Regression** is a supervised learning method used to model the relationship between a **dependent variable** (target) and one or more **independent variables** (features). The model is typically expressed as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w}$ is the vector of weights (or coefficients), \n",
    "- $(b)$ is the bias (or intercept),\n",
    "- $({x})$ is the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60d2e5-aace-402b-bd8e-08ee1a293ffe",
   "metadata": {},
   "source": [
    "## 2. Objective Function (Cost Function)\n",
    "A commonly used cost function for Linear Regression is the **Mean Squared Error (MSE)**:\n",
    "$$\n",
    "\\text{MSE}(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y^{(i)} - \\hat{y}^{(i)}\\right)^2,\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where:\n",
    "- $N$ is the number of training examples,\n",
    "- $y^{(i)}$ is the actual target value for the $i$-th sample,\n",
    "- $\\hat{y}^{(i)}$ = $\\mathbf{w}^T \\mathbf{x}^{(i)} + b$ is the predicted value for the $(i)$-th sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2e69b-5eac-49c2-b13f-96f79e187b08",
   "metadata": {},
   "source": [
    "## 3. Parameter Estimation\n",
    "There are two primary methods to estimate the parameters $\\mathbf{w}$ and $(b)$:\n",
    "\n",
    "1. **Analytical Solution (Normal Equation)**\n",
    "\n",
    "   $$\n",
    "   \\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y},\n",
    "   $$\n",
    "   - This provides a closed-form solution but can be computationally expensive when the number of features is very large.\n",
    "\n",
    "2. **Gradient Descent**  \n",
    "   - **Batch Gradient Descent**: Update parameters using the entire training set for each step.  \n",
    "   - **Stochastic/Mini-batch Gradient Descent**: Update parameters using one sample (SGD) or a small batch of samples (mini-batch) at a time. This often converges faster for large datasets.\n",
    "\n",
    "\n",
    "3. In linear regression we optimize for the **intercept** and **coefficients** of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c10431-d4c8-4452-bbce-d616e5da6d62",
   "metadata": {},
   "source": [
    "## 4. Assumptions of Linear Regression\n",
    "1. **Linearity**: The relationship between features and the target is (approximately) linear.  \n",
    "2. **Independence**: Observations are independent (no autocorrelation).  \n",
    "3. **Homoscedasticity**: The variance of errors is constant across all levels of the independent variables.  \n",
    "4. **Normality of Residuals**: Residuals (errors) are normally distributed.  \n",
    "5. **Low Multicollinearity**: Features should not be excessively correlated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b67f0-b10a-470a-afd9-7c384b6ea8bb",
   "metadata": {},
   "source": [
    "## 5. Common Interview Questions\n",
    "1. **Explain the cost function**: Usually MSE, and why it is preferred (differentiability, etc.).  \n",
    "2. **What is Gradient Descent?**: How it minimizes the cost function, role of the learning rate, and convergence criteria.  \n",
    "3. **Assumptions of Linear Regression**: (listed above).  \n",
    "4. **How to handle overfitting?**: Regularization (L1/Lasso, L2/Ridge), cross-validation, feature selection.  \n",
    "5. **Evaluation metrics**: MSE, RMSE, MAE, $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf684383-d3b3-4eab-a311-ad74c0efc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Инициализация гиперпараметров модели.\n",
    "        :param learning_rate: скорость обучения (шаг градиентного спуска)\n",
    "        :param n_iterations: количество итераций (эпох)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None  # Весовые коэффициенты\n",
    "        self.bias = None     # Смещение (bias)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение (тренировка) линейной регрессии методом градиентного спуска.\n",
    "        :param X: матрица входных данных формы (N, d), где\n",
    "                  N - количество наблюдений (образцов),\n",
    "                  d - количество признаков (фичей).\n",
    "        :param y: вектор целевых значений формы (N,).\n",
    "        \"\"\"\n",
    "        # Считываем количество образцов (N) и количество признаков (d)\n",
    "        N, d = X.shape\n",
    "\n",
    "        # Инициализация весов и смещения нулями\n",
    "        self.weights = np.zeros(d)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        # Цикл по количеству итераций\n",
    "        for _ in range(self.n_iterations):\n",
    "            # 1) Считаем предсказание: y_pred = X @ weights + bias\n",
    "            #   np.dot(X, self.weights) -- это матричное умножение\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            # 2) Вычисляем градиент ошибки по весам:\n",
    "            #   dw = -(2/N) * X^T * (y - y_pred)\n",
    "            dw = -(2 / N) * np.dot(X.T, (y - y_pred))\n",
    "\n",
    "            #   Градиент ошибки по смещению:\n",
    "            #   db = -(2/N) * сумма(y - y_pred)\n",
    "            db = -(2 / N) * np.sum(y - y_pred)\n",
    "\n",
    "            # 3) Обновляем веса и смещение:\n",
    "            #   weight(t+1) = weight(t) - learning_rate * dw\n",
    "            #   bias(t+1)   = bias(t)   - learning_rate * db\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание целевой переменной на основе обученной модели.\n",
    "        :param X: матрица входных данных формы (N, d).\n",
    "        :return: вектор предсказаний y_pred формы (N,).\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11be11a2-b23a-4a07-938f-47917afcb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики (рассчитываем вручную)\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Среднеквадратичная ошибка (MSE)\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Средняя абсолютная ошибка (MAE)\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Коэффициент детерминации (R^2).\n",
    "    Формула: R^2 = 1 - (SS_res / SS_tot),\n",
    "    где SS_res = сумма (y_true - y_pred)^2,\n",
    "        SS_tot = сумма (y_true - среднее(y_true))^2.\n",
    "    :param y_true: вектор истинных значений\n",
    "    :param y_pred: вектор предсказаний\n",
    "    :return: одно число (float), значение R^2.\n",
    "    \"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Корень из среднеквадратичной ошибки (RMSE)\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a2ed66-5575-4152-8d11-7fd0e7fdfd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значения весов (weights): [3.16933339 0.17747302]\n",
      "Смещение (bias): 3.7722722633939476\n",
      "MSE: 0.9813829922788829\n",
      "RMSE: 0.9906477639801561\n",
      "MAE: 0.7886765205572707\n",
      "R^2: 0.7907056161217358\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Генерируем синтетические данные:\n",
    "    # Предположим, у нас один признак (d=1), N=100 выборок\n",
    "    X = 2 * np.random.rand(100, 2)  # X.shape = (100, 1)\n",
    "    y = 4 + 3 * X[:, 0] + np.random.randn(100)  # y.shape = (100,)\n",
    "\n",
    "    # Создаём объект модели LinearRegressionScratch\n",
    "    model = LinearRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
    "\n",
    "    # Обучаем модель\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Делаем предсказания на обучающем наборе\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Вычисляем метрики\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = root_mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    # Выводим результаты\n",
    "    print(\"Значения весов (weights):\", model.weights)\n",
    "    print(\"Смещение (bias):\", model.bias)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R^2:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
